---
title: "Untitled"
output: html_document
---

```{r}
library(readr)
br<- read_csv("Desktop/UChicago/Notion_EDA/brca_data.csv")
dim(br)
```
```{r}
table(br$vital.status)
```
## Question 1
```{r}
set.seed(1)
library(glmnet)
lasso.fit <- cv.glmnet(x = data.matrix(br[, 1:604]), 
                         y = br$vital.status, 
                         family = "binomial")
plot(lasso.fit)
```
```{r}
#getting the best beta
bestbeta = coef(lasso.fit, s = "lambda.min")

# number of nonzero parameters
sum(as.numeric(bestbeta)!=0)
```
```{r}
# the list of nonzero parameters
selected = data.frame("gene names" = rownames(bestbeta), 
                      "beta" = as.numeric(bestbeta))[as.numeric(bestbeta)!=0, ]

# sort
selected[order(abs(selected$beta), decreasing = TRUE), ]
```
The largest parameters are rs_APOB, rs_PIK3C2G and rs_ANO3. However, this is highly unstable because 
1) the Lasso solution may not perform well in data with high colinearity, and 
2) the data is very unbalanced, and there could be overfitting in certain folds.
```{r}
prob=predict(lasso.fit, newx = data.matrix(br[, 1:604]),
                 s = "lambda.min", type = "response")
table(prob > 0.5, as.factor(br$vital.status))
```
In this case, 0.5 would not be a good cut-off point because the data is highly unbalanced. 
Very few observations are predicted to 1.

## Question 2
```{r}
  # the confusion table
table(prob > 0.2, as.factor(br$vital.status))
```
```{r}
  # calculating the sensitivity by hand
35 / (59 + 35)
```

```{r}
# if we use a package
library(caret)
```
```{r}
sensitivity(as.factor( (prob > 0.2)+1-1 ), 
            as.factor(br$vital.status), positive = "1")
```
```{r}
library(ROCR)
roc <- prediction(prob, br$vital.status)

# calculates the ROC curve
perf <- performance(roc,"tpr","fpr")

# the ROC curve
plot(perf,colorize=TRUE)
```
```{r}
# the AUC
performance(roc, measure = "auc")@y.values[[1]]
```

## Question3
Our previous model maybe unstable since the Lasso penalty is vulnerable to colinearity. 
Let’s consider using the elastic-net penalty. 
For this question, you need to consider a grid of α values seq(0, 1, 0.2). 
Hence both Lasso and Ridge regressions are included in the choice. 

- What are the best α and λ values?
- Evaluate you best model by providing the ROC curve plot
- Report the AUC
```{r}
all_alpha = seq(0, 1, 0.2)
bestcvm = Inf
bestalpha = NA

for (alp in all_alpha)
{
  lasso.fit <- cv.glmnet(x = data.matrix(br[, 1:604]), 
                         y = br$vital.status, 
                         family = "binomial", alpha = alp)
  
  # record the best 
  if (min(lasso.fit$cvm) < bestcvm)
  {
    bestfit = lasso.fit
    bestcvm = min(lasso.fit$cvm)
    bestalpha = alp
  }
}
```
```{r}
bestalpha
```
Our best α value is 0. We can evaluate this model:
```{r}
prob = predict(bestfit, newx = data.matrix(br[, 1:604]),
               s = "lambda.min", type = "response")
roc <- prediction(prob, br$vital.status)

# calculates the ROC curve
perf <- performance(roc,"tpr","fpr")

# the ROC curve
plot(perf,colorize=TRUE)
```
```{r}
# the AUC
performance(roc, measure = "auc")@y.values[[1]]
```
The Ridge regression performs the best. This is possibly because high colinearity.
